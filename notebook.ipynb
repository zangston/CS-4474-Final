{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "X_data = data.iloc[:,:-1]\n",
    "Y_data = data.iloc[:,-1:]\n",
    "\n",
    "#print(X_data)\n",
    "#print(Y_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Preprocessing\n",
    "Our selection for the preprocessing of the \"Malicious and Benign Websites\" dataset is OneHotEncoding. The dataset contains mainy columns that contain categorical data, such as \"CHARSET\", which contains data like \"ascii\" or \"UTF-8\". OrdinalEncoding is another option for categorical data; however, it is a better fit when there is some sort of ordering to the data -- such as the categories being \"small\", \"medium\", \"large\" -- and is not as good of a fit for data with no ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1574)\t1.0\n",
      "  (0, 1781)\t1.0\n",
      "  (0, 1925)\t1.0\n",
      "  (0, 1958)\t1.0\n",
      "  (0, 2163)\t1.0\n",
      "  (0, 2263)\t1.0\n",
      "  (0, 2870)\t1.0\n",
      "  (0, 2988)\t1.0\n",
      "  (0, 3131)\t1.0\n",
      "  (0, 4556)\t1.0\n",
      "  (0, 4564)\t1.0\n",
      "  (0, 4660)\t1.0\n",
      "  (0, 4728)\t1.0\n",
      "  (0, 4808)\t1.0\n",
      "  (0, 5578)\t1.0\n",
      "  (0, 5692)\t1.0\n",
      "  (0, 5954)\t1.0\n",
      "  (0, 6759)\t1.0\n",
      "  (0, 7514)\t1.0\n",
      "  (0, 7619)\t1.0\n",
      "  (1, 796)\t1.0\n",
      "  (1, 1781)\t1.0\n",
      "  (1, 1924)\t1.0\n",
      "  (1, 1957)\t1.0\n",
      "  (1, 2024)\t1.0\n",
      "  :\t:\n",
      "  (1779, 5682)\t1.0\n",
      "  (1779, 5798)\t1.0\n",
      "  (1779, 6683)\t1.0\n",
      "  (1779, 7505)\t1.0\n",
      "  (1779, 7618)\t1.0\n",
      "  (1780, 1246)\t1.0\n",
      "  (1780, 1922)\t1.0\n",
      "  (1780, 1952)\t1.0\n",
      "  (1780, 1960)\t1.0\n",
      "  (1780, 2084)\t1.0\n",
      "  (1780, 2731)\t1.0\n",
      "  (1780, 2883)\t1.0\n",
      "  (1780, 3051)\t1.0\n",
      "  (1780, 3232)\t1.0\n",
      "  (1780, 4220)\t1.0\n",
      "  (1780, 4576)\t1.0\n",
      "  (1780, 4666)\t1.0\n",
      "  (1780, 4737)\t1.0\n",
      "  (1780, 5146)\t1.0\n",
      "  (1780, 5594)\t1.0\n",
      "  (1780, 5710)\t1.0\n",
      "  (1780, 6110)\t1.0\n",
      "  (1780, 7109)\t1.0\n",
      "  (1780, 7530)\t1.0\n",
      "  (1780, 7621)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "enc.fit(X_data)\n",
    "\n",
    "X_encoded = enc.transform(X_data)\n",
    "\n",
    "print(X_encoded)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1266)\t1.0\n",
      "  (0, 1793)\t1.0\n",
      "  (0, 1925)\t1.0\n",
      "  (0, 1959)\t1.0\n",
      "  (0, 2078)\t1.0\n",
      "  (0, 2276)\t1.0\n",
      "  (0, 2870)\t1.0\n",
      "  (0, 2988)\t1.0\n",
      "  (0, 3961)\t1.0\n",
      "  (0, 4556)\t1.0\n",
      "  (0, 4590)\t1.0\n",
      "  (0, 4686)\t1.0\n",
      "  (0, 4730)\t1.0\n",
      "  (0, 5324)\t1.0\n",
      "  (0, 5608)\t1.0\n",
      "  (0, 5730)\t1.0\n",
      "  (0, 6448)\t1.0\n",
      "  (0, 7268)\t1.0\n",
      "  (0, 7544)\t1.0\n",
      "  (0, 7621)\t1.0\n",
      "  (1, 1259)\t1.0\n",
      "  (1, 1811)\t1.0\n",
      "  (1, 1926)\t1.0\n",
      "  (1, 1959)\t1.0\n",
      "  (1, 2111)\t1.0\n",
      "  :\t:\n",
      "  (177, 5759)\t1.0\n",
      "  (177, 6581)\t1.0\n",
      "  (177, 7475)\t1.0\n",
      "  (177, 7583)\t1.0\n",
      "  (177, 7625)\t1.0\n",
      "  (178, 251)\t1.0\n",
      "  (178, 1869)\t1.0\n",
      "  (178, 1935)\t1.0\n",
      "  (178, 1955)\t1.0\n",
      "  (178, 2104)\t1.0\n",
      "  (178, 2204)\t1.0\n",
      "  (178, 2883)\t1.0\n",
      "  (178, 2921)\t1.0\n",
      "  (178, 3882)\t1.0\n",
      "  (178, 4407)\t1.0\n",
      "  (178, 4557)\t1.0\n",
      "  (178, 4660)\t1.0\n",
      "  (178, 4726)\t1.0\n",
      "  (178, 4744)\t1.0\n",
      "  (178, 5569)\t1.0\n",
      "  (178, 5682)\t1.0\n",
      "  (178, 5798)\t1.0\n",
      "  (178, 6683)\t1.0\n",
      "  (178, 7505)\t1.0\n",
      "  (178, 7618)\t1.0       Type\n",
      "118      0\n",
      "772      0\n",
      "437      0\n",
      "179      0\n",
      "616      0\n",
      "...    ...\n",
      "960      0\n",
      "1443     0\n",
      "1744     0\n",
      "493      0\n",
      "1670     0\n",
      "\n",
      "[179 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_trn, x_part, y_trn, y_part = train_test_split(X_encoded, Y_data,train_size=0.8,random_state=71)\n",
    "\n",
    "#print(x_trn,y_trn)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_part,y_part,train_size=0.5, random_state=71)\n",
    "\n",
    "#print(x_test,y_test)\n",
    "#print(x_val,y_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Build Classifiers\n",
    "The classifiers we have chosen to use for this project are SVM and Feed-forward Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accurary with defualt parameters: 0.9497206703910615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_clf = SVC()\n",
    "\n",
    "SVM_clf.fit(x_trn,y_trn)\n",
    "\n",
    "svm_acc = SVM_clf.score(x_val,y_val)\n",
    "\n",
    "print(\"SVM accurary with defualt parameters:\",svm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy with default paramters: 0.9497206703910615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "Neural_clf = MLPClassifier()\n",
    "\n",
    "Neural_clf.fit(x_trn,y_trn)\n",
    "\n",
    "neural_acc = Neural_clf.score(x_val,y_val)\n",
    "\n",
    "print(\"Neural Network accuracy with default paramters:\",neural_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Hyperparameter Tuning\n",
    "For the SVM, the hyperparameters that we will be tuning are the C value, the kernel, and the Gamma value.\n",
    "For the Neural Network, the hyperameters that will be tuning are the alpha value, the learning rate, and the hidden_layer_sizes.\n",
    "\n",
    "For both classifiers, we will be using a grid search to tune the hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
