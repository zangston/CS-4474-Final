{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 4774: Final Project\n",
        "Final project report by Ben Tyeyar (bat8hgp), Nick Teller (nst6tx), and Winston Zhang (wyz5rge)"
      ],
      "metadata": {
        "id": "vWsmkLml192i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxP8lKbluoce"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FpHdAjbiuocm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/zangston/CS-4474-Final/main/dataset.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "X_data = data.iloc[:,:-1]\n",
        "Y_data = data.iloc[:,-1:]\n",
        "\n",
        "#print(X_data)\n",
        "#print(Y_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mStn0iBhuoco"
      },
      "source": [
        "# Section 1: Data Preprocessing\n",
        "Our selection for the preprocessing of the \"Malicious and Benign Websites\" dataset is OneHotEncoding. The dataset contains mainy columns that contain categorical data, such as \"CHARSET\", which contains data like \"ascii\" or \"UTF-8\". OrdinalEncoding is another option for categorical data; however, it is a better fit when there is some sort of ordering to the data -- such as the categories being \"small\", \"medium\", \"large\" -- and is not as good of a fit for data with no ordering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mE2h0BOfuocp",
        "outputId": "1e5d8427-f48c-4d81-bd88-3d1480bf0806",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1574)\t1.0\n",
            "  (0, 1781)\t1.0\n",
            "  (0, 1925)\t1.0\n",
            "  (0, 1958)\t1.0\n",
            "  (0, 2163)\t1.0\n",
            "  (0, 2263)\t1.0\n",
            "  (0, 2870)\t1.0\n",
            "  (0, 2988)\t1.0\n",
            "  (0, 3131)\t1.0\n",
            "  (0, 4556)\t1.0\n",
            "  (0, 4564)\t1.0\n",
            "  (0, 4660)\t1.0\n",
            "  (0, 4728)\t1.0\n",
            "  (0, 4808)\t1.0\n",
            "  (0, 5578)\t1.0\n",
            "  (0, 5692)\t1.0\n",
            "  (0, 5954)\t1.0\n",
            "  (0, 6759)\t1.0\n",
            "  (0, 7514)\t1.0\n",
            "  (0, 7619)\t1.0\n",
            "  (1, 796)\t1.0\n",
            "  (1, 1781)\t1.0\n",
            "  (1, 1924)\t1.0\n",
            "  (1, 1957)\t1.0\n",
            "  (1, 2024)\t1.0\n",
            "  :\t:\n",
            "  (1779, 5682)\t1.0\n",
            "  (1779, 5798)\t1.0\n",
            "  (1779, 6683)\t1.0\n",
            "  (1779, 7505)\t1.0\n",
            "  (1779, 7618)\t1.0\n",
            "  (1780, 1246)\t1.0\n",
            "  (1780, 1922)\t1.0\n",
            "  (1780, 1952)\t1.0\n",
            "  (1780, 1960)\t1.0\n",
            "  (1780, 2084)\t1.0\n",
            "  (1780, 2731)\t1.0\n",
            "  (1780, 2883)\t1.0\n",
            "  (1780, 3051)\t1.0\n",
            "  (1780, 3232)\t1.0\n",
            "  (1780, 4220)\t1.0\n",
            "  (1780, 4576)\t1.0\n",
            "  (1780, 4666)\t1.0\n",
            "  (1780, 4737)\t1.0\n",
            "  (1780, 5146)\t1.0\n",
            "  (1780, 5594)\t1.0\n",
            "  (1780, 5710)\t1.0\n",
            "  (1780, 6110)\t1.0\n",
            "  (1780, 7109)\t1.0\n",
            "  (1780, 7530)\t1.0\n",
            "  (1780, 7621)\t1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "enc.fit(X_data)\n",
        "\n",
        "X_encoded = enc.transform(X_data)\n",
        "\n",
        "print(X_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5WVRgytuocr"
      },
      "source": [
        "# Section 2: Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eQuUjBXIuocs",
        "outputId": "566decba-fbf4-4cab-a233-35e71efece35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1036)\t1.0\n",
            "  (0, 1802)\t1.0\n",
            "  (0, 1926)\t1.0\n",
            "  (0, 1957)\t1.0\n",
            "  (0, 2163)\t1.0\n",
            "  (0, 2840)\t1.0\n",
            "  (0, 2883)\t1.0\n",
            "  (0, 2903)\t1.0\n",
            "  (0, 3335)\t1.0\n",
            "  (0, 4167)\t1.0\n",
            "  (0, 4601)\t1.0\n",
            "  (0, 4660)\t1.0\n",
            "  (0, 4730)\t1.0\n",
            "  (0, 5340)\t1.0\n",
            "  (0, 5617)\t1.0\n",
            "  (0, 5730)\t1.0\n",
            "  (0, 6649)\t1.0\n",
            "  (0, 7269)\t1.0\n",
            "  (0, 7553)\t1.0\n",
            "  (0, 7620)\t1.0\n",
            "  (1, 90)\t1.0\n",
            "  (1, 1813)\t1.0\n",
            "  (1, 1929)\t1.0\n",
            "  (1, 1957)\t1.0\n",
            "  (1, 2163)\t1.0\n",
            "  :\t:\n",
            "  (1422, 5714)\t1.0\n",
            "  (1422, 6341)\t1.0\n",
            "  (1422, 7250)\t1.0\n",
            "  (1422, 7539)\t1.0\n",
            "  (1422, 7622)\t1.0\n",
            "  (1423, 108)\t1.0\n",
            "  (1423, 1823)\t1.0\n",
            "  (1423, 1926)\t1.0\n",
            "  (1423, 1957)\t1.0\n",
            "  (1423, 1970)\t1.0\n",
            "  (1423, 2213)\t1.0\n",
            "  (1423, 2883)\t1.0\n",
            "  (1423, 2948)\t1.0\n",
            "  (1423, 3171)\t1.0\n",
            "  (1423, 4274)\t1.0\n",
            "  (1423, 4557)\t1.0\n",
            "  (1423, 4660)\t1.0\n",
            "  (1423, 4726)\t1.0\n",
            "  (1423, 4744)\t1.0\n",
            "  (1423, 5569)\t1.0\n",
            "  (1423, 5682)\t1.0\n",
            "  (1423, 5798)\t1.0\n",
            "  (1423, 6683)\t1.0\n",
            "  (1423, 7505)\t1.0\n",
            "  (1423, 7618)\t1.0       Type\n",
            "365      0\n",
            "828      0\n",
            "1547     0\n",
            "1237     0\n",
            "1137     0\n",
            "...    ...\n",
            "1320     0\n",
            "1576     0\n",
            "1079     0\n",
            "1035     1\n",
            "1131     0\n",
            "\n",
            "[1424 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_trn, x_part, y_trn, y_part = train_test_split(X_encoded, Y_data,train_size=0.8,random_state=71)\n",
        "\n",
        "print(x_trn,y_trn)\n",
        "\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_part,y_part,train_size=0.5, random_state=71)\n",
        "\n",
        "#print(x_test,y_test)\n",
        "#print(x_val,y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxeVyr_Buocu"
      },
      "source": [
        "# Section 3: Build Classifiers\n",
        "The classifiers we have chosen to use for this project are SVM and Feed-forward Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VMq7T4hluocw",
        "outputId": "9da9566c-0f11-4dec-910c-bb97845bdfe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM accurary with defualt parameters: 0.9497206703910615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "SVM_clf = SVC()\n",
        "\n",
        "SVM_clf.fit(x_trn,y_trn)\n",
        "\n",
        "svm_acc = SVM_clf.score(x_val,y_val)\n",
        "\n",
        "print(\"SVM accurary with default parameters:\",svm_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-vBF1rPQuocy",
        "outputId": "97da0ba3-eb5f-42f8-8018-eca392d693dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network accuracy with default paramters: 0.9385474860335196\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "Neural_clf = MLPClassifier()\n",
        "\n",
        "Neural_clf.fit(x_trn,y_trn)\n",
        "\n",
        "neural_acc = Neural_clf.score(x_val,y_val)\n",
        "\n",
        "print(\"Neural Network accuracy with default paramters:\",neural_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axg9acOluocz"
      },
      "source": [
        "# Section 4: Hyperparameter Tuning\n",
        "For the SVM, the hyperparameters that we will be tuning are the C value, the kernel, and the Gamma value.\n",
        "For the Neural Network, the hyperameters that will be tuning are the alpha value, the learning rate, and the hidden_layer_sizes.\n",
        "\n",
        "For both classifiers, we will be using a grid search to tune the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14vZtqZWuoc2",
        "outputId": "27ebfe9c-d0bc-45a3-9374-cdfe4107cf4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='A column-vector y was passed when a 1d array was expected.*')\n",
        "param_grid = {'C': [0.1, 1, 10, 100],\n",
        "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "              'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 7))}\n",
        "\n",
        "grid_search = GridSearchCV(SVC(),param_grid,cv=5)\n",
        "grid_search.fit(x_trn,y_trn)\n",
        "\n",
        "print(grid_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wtFIfHBuoc2",
        "outputId": "f31b7856-26ed-48ea-d0ff-c81a7eaf3eda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "C:\\Users\\benty\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha': 1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "params = {'alpha': [0.01, 0.1, 1],\n",
        "          'learning_rate': ['constant', 'adaptive'],\n",
        "          'hidden_layer_sizes': [(10,), (50,), (100,), (10,10), (50,50), (100,100)]}\n",
        "\n",
        "grid_search = GridSearchCV(MLPClassifier(),params,cv=5)\n",
        "grid_search.fit(x_trn,y_trn)\n",
        "\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Anaylsis\n",
        "**Based on the results above, which classifier is better, and why?**\n",
        "\n",
        "Based on the accuracy scores computed after building our classifiers in Section 3, we observe an accuracy score of 94.97% for the SVM and 93.85% for the neural network. This means that the SVM is the better classifier as it performed slightly better than the neural network. This may be because of the categorical nature of the data, which the SVM, who uses a hyperplane to classify data, was slightly better equipped to process. SVMs are thus known to work well with datasets with many dimension. Neural networks, on the other hand, struggle with such dimensionality.\n",
        "\n",
        "**For further improvement on classification accuracy, what strategies can you use and why do you think they will be helpful?**\n",
        "\n",
        "One strategy that could be used is to increase the size of the dataset. Obviously, giving the classifier more data will allow it to have a better fit. This is because the Law of Large Numbers dictates that such functions will converge to an average form as the sample size grows large. As all statisticians and data scientists know, while this is the simplest and easy solution, this might not always be possible, as data collection can become a very expensive process.\n",
        "\n",
        "Another strategy is to use ensemble methods, e.g., bagging or boosting. As suggested by the name, ensemble methods combine the predictions of several classifiers. When we elect to do this, we are less constrained by the shortcomings of any one classifier, while benefitting from the merits of many. \"We get the best of both worlds\".\n",
        "\n",
        "**Which input feature is the most informative one, and what is your empirical evidence?**"
      ],
      "metadata": {
        "id": "tqAL-cGSz-lU"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}